### 来自[dtm](https://dtm.pub/ref/gozero.html)的灵感，基于redis分布式锁的分布式事务。

常见的CAP理论中，C 一致性，A 可用性，P 分区容错性，一般保证P和A，舍弃C，以及最终一致性，本文实现了AP和最终一致性。并给补偿操作有重试机制。

之前也是学习了DTM，但是发现一个问题，在高并发情况下，dtm事务并不稳定，补偿操作不成功，已过多次验证，如下

- 这里我准备一个每秒1000并发的dtm压测，

![image-20241214200601723](images\image-20241214200601723.png)

- stock的库存为100个，成功执行的话会减1库存，回滚补偿就会加1

![image-20241214200241149](images\image-20241214200241149.png)

- order这里插入成功的话就是新建一条记录，然后row_state的值是 0，回滚补偿了row_state是-1

  ![image-20241214200354019](images\image-20241214200354019.png)

- 看压测结果，成功217个

![image-20241214200534660](images\image-20241214200534660.png)

- stock没有问题，成功扣除完成

![image-20241214200706228](images\image-20241214200706228.png)

- 但是order出问题了，插入了大量的成功状态的记录，大量补偿失败了

![image-20241214201343030](images\image-20241214201343030.png)

#### 于是我打算自己写一个基于redis分布式锁的分布式事务，防止补偿失败的问题。

这个事务目前可以提供2个微服务的一致性，需要2个以上的我感觉可以先合并下服务

具体时序图下

![image-20241214202609524](images\image-20241214202609524.png)

采用两个err分别表示确认微服务的状态

1. Create(l.ctx, createOrderReq)如果失败了，服务直接返回失败，啥没发生
2. Create(l.ctx, createOrderReq)成功了，row_state为0，但是l.svcCtx.StockRpc.Deduct(l.ctx, deductReq)失败，就会进行补偿order，补偿加入了重试
3. 两个微服务都加入了分布式锁+uid的方法，防止延迟导致的误删。

- 进行压测，可以看到由于锁的存在1，成功数只有4了

  ![image-20241214204252365](images\image-20241214204252365.png)

但是，order和stock不在出现补偿失败导致的不一致的情况。扣除4个，4个0状态

![image-20241214204419091](images\image-20241214204419091.png)

![image-20241214204433122](images\image-20241214204433122.png)

多次压测的结果一致。

![image-20241214204604732](images\image-20241214204604732.png)

![image-20241214204613884](images\image-20241214204613884.png)

![image-20241214204626565](images\image-20241214204626565.png)

压测为5000qps下，最终一致性也是可以保证的

![image-20241214210804031](images\image-20241214210804031.png)

![image-20241214210811660](images\image-20241214210811660.png)

![image-20241214210819236](images\image-20241214210819236.png)